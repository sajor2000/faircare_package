"""
Tests for FairCareAI Van Calster Performance Metrics Module.

These tests verify the implementation of the four key performance measures
recommended by Van Calster et al. (2025) Lancet Digital Health:
1. AUROC by subgroup (discrimination)
2. Calibration by subgroup (detecting differential miscalibration)
3. Net benefit by subgroup (clinical utility)
4. Risk distribution by subgroup (probability distributions)

Reference:
    Van Calster B, Collins GS, Vickers AJ, et al. Evaluation of performance
    measures in predictive AI models to support medical decisions.
    Lancet Digit Health 2025. doi:10.1016/j.landig.2025.100916
"""

import numpy as np
import polars as pl
import pytest

from faircareai.metrics.vancalster import (
    AUROC_DIFF_CLINICALLY_MEANINGFUL,
    compute_auroc_by_subgroup,
    compute_calibration_by_subgroup,
    compute_net_benefit_by_subgroup,
    compute_risk_distribution_by_subgroup,
    compute_vancalster_metrics,
)

# =============================================================================
# FIXTURES
# =============================================================================


@pytest.fixture
def binary_classification_data() -> pl.DataFrame:
    """Create sample data for binary classification with subgroups."""
    np.random.seed(42)
    n = 500

    # Create demographic groups
    race = ["White"] * 250 + ["Black"] * 150 + ["Hispanic"] * 100

    # Create risk scores with controlled properties
    # White group: well-calibrated
    risk_white = np.random.beta(3, 7, 250)
    outcomes_white = (np.random.random(250) < risk_white).astype(int)

    # Black group: slightly higher risk
    risk_black = np.random.beta(4, 6, 150)
    outcomes_black = (np.random.random(150) < risk_black).astype(int)

    # Hispanic group: similar to White
    risk_hispanic = np.random.beta(3, 7, 100)
    outcomes_hispanic = (np.random.random(100) < risk_hispanic).astype(int)

    return pl.DataFrame(
        {
            "risk_score": np.concatenate([risk_white, risk_black, risk_hispanic]),
            "outcome": np.concatenate([outcomes_white, outcomes_black, outcomes_hispanic]),
            "race": race,
        }
    )


@pytest.fixture
def perfect_discrimination_data() -> pl.DataFrame:
    """Create data with perfect discrimination (AUROC = 1.0)."""
    np.random.seed(42)
    n_per_outcome = 100

    # Events have high risk scores
    risk_events = np.random.uniform(0.8, 1.0, n_per_outcome)
    # Non-events have low risk scores
    risk_nonevents = np.random.uniform(0.0, 0.2, n_per_outcome)

    return pl.DataFrame(
        {
            "risk_score": np.concatenate([risk_events, risk_nonevents]),
            "outcome": np.array([1] * n_per_outcome + [0] * n_per_outcome),
            "group": ["A"] * n_per_outcome + ["B"] * n_per_outcome,
        }
    )


@pytest.fixture
def miscalibrated_data() -> pl.DataFrame:
    """Create data with intentional miscalibration."""
    np.random.seed(42)
    n = 300

    # Predictions systematically overestimate risk
    y_prob = np.random.uniform(0.5, 0.9, n)
    # True rate is lower
    y_true = (np.random.random(n) < 0.2).astype(int)

    return pl.DataFrame(
        {
            "risk_score": y_prob,
            "outcome": y_true,
            "group": ["A"] * 150 + ["B"] * 150,
        }
    )


@pytest.fixture
def differential_performance_data() -> pl.DataFrame:
    """Create data with different performance across groups."""
    np.random.seed(42)

    # Group A: good performance
    n_a = 200
    risk_a = np.random.beta(2, 5, n_a)
    outcomes_a = (np.random.random(n_a) < risk_a).astype(int)

    # Group B: poor performance (random predictions)
    n_b = 200
    risk_b = np.random.uniform(0, 1, n_b)
    outcomes_b = np.random.binomial(1, 0.3, n_b)

    return pl.DataFrame(
        {
            "risk_score": np.concatenate([risk_a, risk_b]),
            "outcome": np.concatenate([outcomes_a, outcomes_b]),
            "group": ["A"] * n_a + ["B"] * n_b,
        }
    )


# =============================================================================
# TEST: compute_vancalster_metrics (PRIMARY ENTRY POINT)
# =============================================================================


class TestComputeVancalsterMetrics:
    """Tests for the main compute_vancalster_metrics function."""

    def test_returns_all_required_keys(self, binary_classification_data: pl.DataFrame) -> None:
        """Should return dict with all required top-level keys."""
        result = compute_vancalster_metrics(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            bootstrap_ci=False,
        )

        assert "citation" in result
        assert "methodology" in result
        assert "overall" in result
        assert "by_subgroup" in result
        assert "disparities" in result
        assert "interpretation" in result
        assert "Van Calster" in result["citation"]

    def test_overall_metrics_complete(self, binary_classification_data: pl.DataFrame) -> None:
        """Overall metrics should contain all four Van Calster domains."""
        result = compute_vancalster_metrics(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            bootstrap_ci=False,
        )

        overall = result["overall"]
        assert "discrimination" in overall
        assert "calibration" in overall
        assert "clinical_utility" in overall
        assert "risk_distribution" in overall

    def test_subgroup_metrics_per_group(self, binary_classification_data: pl.DataFrame) -> None:
        """Should compute metrics for each subgroup."""
        result = compute_vancalster_metrics(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            bootstrap_ci=False,
        )

        subgroups = result["by_subgroup"]
        assert "White" in subgroups
        assert "Black" in subgroups
        assert "Hispanic" in subgroups

        # Each subgroup should have all four domains
        for group_name, group_data in subgroups.items():
            assert "discrimination" in group_data
            assert "calibration" in group_data
            assert "clinical_utility" in group_data
            assert "risk_distribution" in group_data

    def test_reference_group_identification(self, binary_classification_data: pl.DataFrame) -> None:
        """Should identify largest group as reference by default."""
        result = compute_vancalster_metrics(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            bootstrap_ci=False,
        )

        # White has most samples (250), should be reference
        assert result["reference_group"] == "White"
        assert result["by_subgroup"]["White"]["is_reference"] is True
        assert result["by_subgroup"]["Black"]["is_reference"] is False

    def test_custom_reference_group(self, binary_classification_data: pl.DataFrame) -> None:
        """Should allow specifying custom reference group."""
        result = compute_vancalster_metrics(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            reference="Black",
            bootstrap_ci=False,
        )

        assert result["reference_group"] == "Black"
        assert result["by_subgroup"]["Black"]["is_reference"] is True

    def test_disparities_computed(self, binary_classification_data: pl.DataFrame) -> None:
        """Should compute disparities vs reference group."""
        result = compute_vancalster_metrics(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            bootstrap_ci=False,
        )

        disparities = result["disparities"]
        assert "reference" in disparities
        assert "comparisons" in disparities

        # Should have comparisons for non-reference groups
        comparisons = disparities["comparisons"]
        assert "Black" in comparisons
        assert "Hispanic" in comparisons
        assert "White" not in comparisons  # Reference not compared to itself

    def test_bootstrap_ci_computation(self, binary_classification_data: pl.DataFrame) -> None:
        """Should compute bootstrap CIs when requested."""
        result = compute_vancalster_metrics(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            bootstrap_ci=True,
            n_bootstrap=50,  # Small for test speed
        )

        discrimination = result["overall"]["discrimination"]
        assert "auroc_ci_95" in discrimination
        assert len(discrimination["auroc_ci_95"]) == 2
        assert (
            discrimination["auroc_ci_95"][0]
            <= discrimination["auroc"]
            <= discrimination["auroc_ci_95"][1]
        )

    def test_without_group_column(self, binary_classification_data: pl.DataFrame) -> None:
        """Should work without subgroup analysis (overall only)."""
        result = compute_vancalster_metrics(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col=None,
            bootstrap_ci=False,
        )

        assert "overall" in result
        assert "by_subgroup" not in result
        assert "disparities" not in result


# =============================================================================
# TEST: AUROC BY SUBGROUP
# =============================================================================


class TestAUROCBySubgroup:
    """Tests for AUROC computation across subgroups."""

    def test_basic_auroc_computation(self, binary_classification_data: pl.DataFrame) -> None:
        """Should compute valid AUROC values for each group."""
        result = compute_auroc_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            bootstrap_ci=False,
        )

        assert "groups" in result
        for group_data in result["groups"].values():
            auroc = group_data.get("auroc")
            assert auroc is not None
            assert 0.0 <= auroc <= 1.0

    def test_perfect_discrimination_auroc(self, perfect_discrimination_data: pl.DataFrame) -> None:
        """Perfect separation should yield AUROC near 1.0."""
        result = compute_auroc_by_subgroup(
            perfect_discrimination_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="group",
            bootstrap_ci=False,
        )

        # Note: Each group only has one outcome class, so AUROC is undefined per-group
        # This tests overall behavior with perfect discrimination in combined data

    def test_auroc_disparities_flagged(self, differential_performance_data: pl.DataFrame) -> None:
        """Should flag clinically meaningful AUROC differences."""
        result = compute_auroc_by_subgroup(
            differential_performance_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="group",
            bootstrap_ci=False,
        )

        disparities = result["disparities"]
        # Group B (random) should have worse AUROC than Group A
        if "B" in disparities:
            assert "auroc_diff" in disparities["B"]
            assert "clinically_meaningful" in disparities["B"]

    def test_auroc_interpretation_included(self, binary_classification_data: pl.DataFrame) -> None:
        """Should include interpretation for each group."""
        result = compute_auroc_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            bootstrap_ci=False,
        )

        for group_data in result["groups"].values():
            assert "interpretation" in group_data

    def test_citation_included(self, binary_classification_data: pl.DataFrame) -> None:
        """Should include Van Calster citation."""
        result = compute_auroc_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            bootstrap_ci=False,
        )

        assert "citation" in result
        assert "Van Calster" in result["citation"]


# =============================================================================
# TEST: CALIBRATION BY SUBGROUP
# =============================================================================


class TestCalibrationBySubgroup:
    """Tests for calibration metrics across subgroups."""

    def test_basic_calibration_metrics(self, binary_classification_data: pl.DataFrame) -> None:
        """Should compute standard calibration metrics for each group."""
        result = compute_calibration_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
        )

        for group_data in result["groups"].values():
            if "error" not in group_data:
                assert "brier_score" in group_data
                assert "oe_ratio" in group_data
                assert (
                    "calibration_slope" in group_data or group_data.get("calibration_slope") is None
                )
                assert (
                    "calibration_intercept" in group_data
                    or group_data.get("calibration_intercept") is None
                )

    def test_calibration_curve_data(self, binary_classification_data: pl.DataFrame) -> None:
        """Should include calibration curve data for plotting."""
        result = compute_calibration_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
        )

        for group_data in result["groups"].values():
            if "error" not in group_data:
                cal_curve = group_data.get("calibration_curve")
                if cal_curve is not None:
                    assert "prob_true" in cal_curve
                    assert "prob_pred" in cal_curve
                    assert len(cal_curve["prob_true"]) == len(cal_curve["prob_pred"])

    def test_miscalibration_detected(self, miscalibrated_data: pl.DataFrame) -> None:
        """Should detect systematic miscalibration."""
        result = compute_calibration_by_subgroup(
            miscalibrated_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="group",
        )

        for group_data in result["groups"].values():
            if "error" not in group_data:
                # O:E ratio should indicate overprediction
                oe = group_data.get("oe_ratio")
                if oe is not None:
                    # Model predicts 0.5-0.9, true rate ~0.2
                    # O:E = 0.2 / 0.7 â‰ˆ 0.29 (< 1 indicates overprediction)
                    assert oe < 1.0

    def test_brier_score_range(self, binary_classification_data: pl.DataFrame) -> None:
        """Brier score should be in valid range [0, 1]."""
        result = compute_calibration_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
        )

        for group_data in result["groups"].values():
            if "error" not in group_data:
                brier = group_data.get("brier_score")
                if brier is not None:
                    assert 0.0 <= brier <= 1.0


# =============================================================================
# TEST: NET BENEFIT BY SUBGROUP
# =============================================================================


class TestNetBenefitBySubgroup:
    """Tests for net benefit and decision curve analysis by subgroup."""

    def test_basic_net_benefit_computation(self, binary_classification_data: pl.DataFrame) -> None:
        """Should compute net benefit for each group."""
        result = compute_net_benefit_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            threshold=0.3,
        )

        for group_data in result["groups"].values():
            assert "net_benefit" in group_data
            assert "net_benefit_max" in group_data
            assert "prevalence" in group_data

    def test_decision_curve_data(self, binary_classification_data: pl.DataFrame) -> None:
        """Should include decision curve data for plotting."""
        result = compute_net_benefit_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
        )

        for group_data in result["groups"].values():
            dc = group_data.get("decision_curve")
            assert dc is not None
            assert "thresholds" in dc
            assert "net_benefit_model" in dc
            assert "net_benefit_all" in dc
            assert "net_benefit_none" in dc

    def test_useful_range_identified(self, binary_classification_data: pl.DataFrame) -> None:
        """Should identify useful threshold range."""
        result = compute_net_benefit_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
        )

        for group_data in result["groups"].values():
            useful = group_data.get("useful_range")
            assert useful is not None
            if useful["min"] is not None:
                assert useful["min"] < useful["max"]

    def test_standardized_net_benefit(self, binary_classification_data: pl.DataFrame) -> None:
        """Should compute standardized net benefit (NB / prevalence)."""
        result = compute_net_benefit_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
        )

        for group_data in result["groups"].values():
            snb = group_data.get("standardized_net_benefit")
            if snb is not None:
                # Standardized NB can exceed 1.0 in some cases
                assert isinstance(snb, float)

    def test_threshold_in_result(self, binary_classification_data: pl.DataFrame) -> None:
        """Should record the primary threshold used."""
        result = compute_net_benefit_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            threshold=0.25,
        )

        assert result["primary_threshold"] == 0.25


# =============================================================================
# TEST: RISK DISTRIBUTION BY SUBGROUP
# =============================================================================


class TestRiskDistributionBySubgroup:
    """Tests for risk distribution analysis by subgroup."""

    def test_basic_distribution_stats(self, binary_classification_data: pl.DataFrame) -> None:
        """Should compute distribution statistics for events and non-events."""
        result = compute_risk_distribution_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
        )

        for group_data in result["groups"].values():
            # Check events distribution
            events = group_data.get("events", {})
            if events.get("n", 0) > 0:
                assert "mean" in events
                assert "median" in events
                assert "std" in events
                assert "q25" in events
                assert "q75" in events

            # Check non-events distribution
            non_events = group_data.get("non_events", {})
            if non_events.get("n", 0) > 0:
                assert "mean" in non_events
                assert "median" in non_events

    def test_discrimination_slope(self, binary_classification_data: pl.DataFrame) -> None:
        """Should compute discrimination slope (mean events - mean non-events)."""
        result = compute_risk_distribution_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
        )

        for group_data in result["groups"].values():
            disc_slope = group_data.get("discrimination_slope")
            if disc_slope is not None:
                # Events should have higher predicted probability
                assert disc_slope > 0 or True  # May be negative in some edge cases

    def test_histogram_data_for_plotting(self, binary_classification_data: pl.DataFrame) -> None:
        """Should include histogram data for visualization."""
        result = compute_risk_distribution_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
        )

        for group_data in result["groups"].values():
            events = group_data.get("events", {})
            if events.get("n", 0) > 0:
                hist = events.get("histogram")
                if hist is not None:
                    assert "counts" in hist
                    assert "bin_edges" in hist
                    assert "bin_centers" in hist

    def test_ks_test_included(self, binary_classification_data: pl.DataFrame) -> None:
        """Should include KS test for distribution difference."""
        result = compute_risk_distribution_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
        )

        for group_data in result["groups"].values():
            ks_test = group_data.get("ks_test")
            if ks_test is not None:
                assert "statistic" in ks_test
                assert "p_value" in ks_test
                assert 0.0 <= ks_test["p_value"] <= 1.0


# =============================================================================
# TEST: INTERPRETATION AND CLINICAL GUIDANCE
# =============================================================================


class TestInterpretation:
    """Tests for clinical interpretation generation."""

    def test_interpretation_structure(self, binary_classification_data: pl.DataFrame) -> None:
        """Interpretation should have required structure."""
        result = compute_vancalster_metrics(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            bootstrap_ci=False,
        )

        interp = result["interpretation"]
        assert "summary" in interp
        assert "concerns" in interp  # Backward compat alias
        assert "recommendations" in interp  # Backward compat alias (now empty)
        assert "finding_count" in interp  # New neutral structure
        assert "subgroup_count" in interp  # New neutral structure

    def test_concerns_flagged_for_disparities(
        self, differential_performance_data: pl.DataFrame
    ) -> None:
        """Should flag findings when performance differs across groups."""
        result = compute_vancalster_metrics(
            differential_performance_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="group",
            bootstrap_ci=False,
        )

        interp = result["interpretation"]
        # With differential performance, should have findings or concerns
        assert interp["finding_count"] >= 0  # Count-based, not status-based
        assert "findings" in interp

    def test_auroc_interpretation_levels(self) -> None:
        """AUROC interpretation should match documented thresholds."""
        # Test interpretation via a minimal dataset
        data = pl.DataFrame(
            {
                "risk_score": [0.1, 0.9, 0.2, 0.8, 0.3, 0.7] * 20,
                "outcome": [0, 1, 0, 1, 0, 1] * 20,
                "group": ["A"] * 120,
            }
        )

        result = compute_auroc_by_subgroup(
            data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="group",
            bootstrap_ci=False,
        )

        interp = result["groups"]["A"]["interpretation"]
        assert isinstance(interp, str)


# =============================================================================
# TEST: EDGE CASES
# =============================================================================


class TestEdgeCases:
    """Tests for edge cases and error handling."""

    def test_small_sample_warning(self) -> None:
        """Should warn when sample size is too small."""
        small_data = pl.DataFrame(
            {
                "risk_score": [0.3, 0.7, 0.5],
                "outcome": [0, 1, 0],
                "group": ["A", "A", "A"],
            }
        )

        result = compute_vancalster_metrics(
            small_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="group",
            bootstrap_ci=False,
        )

        # Should flag small sample size
        group_data = result["by_subgroup"]["A"]
        assert group_data.get("small_sample_warning") is True or "error" in group_data

    def test_single_class_outcome(self) -> None:
        """Should handle groups with single outcome class."""
        single_class_data = pl.DataFrame(
            {
                "risk_score": [0.3, 0.4, 0.5, 0.6] * 10,
                "outcome": [0] * 40,  # All same outcome
                "group": ["A"] * 40,
            }
        )

        result = compute_auroc_by_subgroup(
            single_class_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="group",
            bootstrap_ci=False,
        )

        # Should handle gracefully (AUROC undefined with single class)
        group_a = result["groups"]["A"]
        assert "error" in group_a or group_a.get("auroc") is None

    def test_extreme_probabilities(self) -> None:
        """Should handle probabilities at boundaries (0 and 1)."""
        extreme_data = pl.DataFrame(
            {
                "risk_score": [0.0, 0.0, 1.0, 1.0, 0.5, 0.5] * 20,
                "outcome": [0, 0, 1, 1, 0, 1] * 20,
                "group": ["A"] * 120,
            }
        )

        result = compute_calibration_by_subgroup(
            extreme_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="group",
        )

        # Should not crash with extreme values
        assert "groups" in result

    def test_missing_values_handling(self) -> None:
        """Should handle null/missing values in group column."""
        data_with_nulls = pl.DataFrame(
            {
                "risk_score": [0.3, 0.7, 0.5, 0.4, 0.6],
                "outcome": [0, 1, 0, 1, 0],
                "group": ["A", "A", None, "B", "B"],
            }
        )

        result = compute_auroc_by_subgroup(
            data_with_nulls,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="group",
            bootstrap_ci=False,
        )

        # Should only have A and B, not null
        assert "A" in result["groups"]
        assert "B" in result["groups"]


# =============================================================================
# TEST: CONSTANTS AND CONFIGURATION
# =============================================================================


class TestConstants:
    """Tests for module constants and configuration."""

    def test_clinically_meaningful_threshold(self) -> None:
        """Clinically meaningful AUROC difference should be 0.05 per Van Calster."""
        assert AUROC_DIFF_CLINICALLY_MEANINGFUL == 0.05

    def test_custom_net_benefit_thresholds(self, binary_classification_data: pl.DataFrame) -> None:
        """Should accept custom threshold array for decision curves."""
        custom_thresholds = np.linspace(0.1, 0.5, 10)

        result = compute_net_benefit_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            thresholds=custom_thresholds,
        )

        for group_data in result["groups"].values():
            dc = group_data["decision_curve"]
            assert len(dc["thresholds"]) == 10

    def test_custom_calibration_bins(self, binary_classification_data: pl.DataFrame) -> None:
        """Should accept custom number of calibration bins."""
        result = compute_calibration_by_subgroup(
            binary_classification_data,
            y_prob_col="risk_score",
            y_true_col="outcome",
            group_col="race",
            n_bins=5,
        )

        for group_data in result["groups"].values():
            if "error" not in group_data:
                cal_curve = group_data.get("calibration_curve")
                if cal_curve is not None:
                    assert cal_curve["n_bins"] == 5
