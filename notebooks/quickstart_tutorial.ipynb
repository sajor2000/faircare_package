{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FairCareAI Quick Start Tutorial\n",
    "\n",
    "This notebook demonstrates the complete workflow for auditing ML models for fairness in clinical contexts using FairCareAI.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Loading prediction data\n",
    "2. Detecting and configuring sensitive attributes\n",
    "3. Selecting appropriate fairness metrics\n",
    "4. Running a fairness audit\n",
    "5. Interpreting results\n",
    "6. Generating governance reports\n",
    "\n",
    "## Governance Philosophy\n",
    "\n",
    "> **Package SUGGESTS, humans DECIDE**\n",
    ">\n",
    "> All FairCareAI outputs are ADVISORY. Final deployment decisions rest with clinical stakeholders and governance committees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install FairCareAI if needed\n",
    "# !pip install faircareai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for development\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "from faircareai import FairCareAudit, FairnessConfig\n",
    "from faircareai.core.config import FairnessMetric, UseCaseType, ModelType\n",
    "from faircareai.data.synthetic import generate_icu_mortality_data, get_data_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Data\n",
    "\n",
    "For this tutorial, we'll use FairCareAI's synthetic ICU mortality data generator. In practice, you would load your own predictions.\n",
    "\n",
    "The synthetic data includes:\n",
    "- Model predictions (probabilities)\n",
    "- Actual outcomes\n",
    "- Demographic attributes (race/ethnicity, insurance, language)\n",
    "- Intentional disparities for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic ICU mortality data\n",
    "df = generate_icu_mortality_data(\n",
    "    n_samples=2000,\n",
    "    seed=42,\n",
    "    disparity_strength=0.08,  # 8% TPR gap between groups\n",
    ")\n",
    "\n",
    "# View data summary\n",
    "summary = get_data_summary(df)\n",
    "print(f\"Generated {summary['n_samples']:,} patient records\")\n",
    "print(f\"Mortality rate: {summary['mortality_rate']:.1%}\")\n",
    "print(f\"Prediction rate: {summary['prediction_rate']:.1%}\")\n",
    "\n",
    "# Show first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Audit\n",
    "\n",
    "Create a `FairCareAudit` object with your prediction data. FairCareAI supports:\n",
    "- Polars DataFrames (recommended for large datasets)\n",
    "- Parquet files\n",
    "- CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the audit\n",
    "audit = FairCareAudit(\n",
    "    data=df,\n",
    "    pred_col=\"prediction\",      # Column with model predictions (probabilities)\n",
    "    target_col=\"mortality\",     # Column with actual outcomes (0/1)\n",
    "    threshold=0.5,              # Decision threshold\n",
    ")\n",
    "\n",
    "print(f\"Audit initialized with {len(audit.df):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Detect Sensitive Attributes\n",
    "\n",
    "FairCareAI automatically scans your data for common healthcare demographic columns and suggests which to use for fairness analysis.\n",
    "\n",
    "**Important**: You must explicitly accept suggestions. FairCareAI never assumes which attributes to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See suggested sensitive attributes\n",
    "suggestions = audit.suggest_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept suggestions (1-indexed)\n",
    "# You can select which attributes to include in your audit\n",
    "audit.accept_suggested_attributes([1, 2, 3])  # race_ethnicity, insurance, language\n",
    "\n",
    "print(f\"Configured {len(audit.sensitive_attributes)} sensitive attributes:\")\n",
    "for attr in audit.sensitive_attributes:\n",
    "    print(f\"  - {attr.name} (reference: {attr.reference})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Select Fairness Metric\n",
    "\n",
    "### The Impossibility Theorem\n",
    "\n",
    "Per Chouldechova (2017) and Kleinberg et al. (2017), it is **mathematically impossible** to satisfy all fairness metrics simultaneously when base rates differ between groups.\n",
    "\n",
    "**The choice of fairness metric is a value judgment** that humans must make based on clinical context.\n",
    "\n",
    "### Use-Case Based Recommendations\n",
    "\n",
    "| Use Case | Recommended Metric | Key Concern |\n",
    "|----------|-------------------|-------------|\n",
    "| Intervention Trigger | Equalized Odds | Equal access to beneficial interventions |\n",
    "| Risk Communication | Calibration | Trustworthy probabilities for decisions |\n",
    "| Resource Allocation | Demographic Parity | Proportional resource distribution |\n",
    "| Screening | Equal Opportunity | Equal detection rates for those with disease |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the use case to get recommendations\n",
    "audit.config.use_case_type = UseCaseType.INTERVENTION_TRIGGER\n",
    "\n",
    "# Get fairness metric recommendation\n",
    "recommendation = audit.suggest_fairness_metric()\n",
    "print(\"Recommendation:\")\n",
    "for key, value in recommendation.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure the Full Audit\n",
    "\n",
    "Now we configure the complete audit with all required CHAI criteria:\n",
    "\n",
    "- Model identity (name, version)\n",
    "- Intended use and population\n",
    "- Selected fairness metric with justification\n",
    "\n",
    "**CHAI Requirement**: You must provide a justification for your fairness metric choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the audit\n",
    "audit.config = FairnessConfig(\n",
    "    # Model Identity (CHAI AC1.CR1-4)\n",
    "    model_name=\"ICU Mortality Prediction Model\",\n",
    "    model_version=\"1.0.0\",\n",
    "    model_type=ModelType.BINARY_CLASSIFIER,\n",
    "    \n",
    "    # Intended Use (CHAI AC1.CR1, AC1.CR100)\n",
    "    intended_use=(\n",
    "        \"Trigger early intervention for high-risk ICU patients. \"\n",
    "        \"Model flags patients for enhanced monitoring and care team review.\"\n",
    "    ),\n",
    "    intended_population=\"Adult patients admitted to medical/surgical ICU\",\n",
    "    out_of_scope=[\"Pediatric patients\", \"Cardiac ICU\", \"Burn unit\"],\n",
    "    \n",
    "    # Fairness Prioritization (CHAI AC1.CR92-93)\n",
    "    primary_fairness_metric=FairnessMetric.EQUALIZED_ODDS,\n",
    "    fairness_justification=(\n",
    "        \"Model triggers life-saving intervention (enhanced monitoring). \"\n",
    "        \"Equalized odds ensures equal true positive rates across groups, \"\n",
    "        \"preventing differential access to this beneficial intervention. \"\n",
    "        \"Equal false positive rates ensure equal burden of unnecessary alerts.\"\n",
    "    ),\n",
    "    use_case_type=UseCaseType.INTERVENTION_TRIGGER,\n",
    "    \n",
    "    # Report Settings\n",
    "    organization_name=\"Example Health System\",\n",
    "    include_chai_mapping=True,\n",
    ")\n",
    "\n",
    "# Validate configuration\n",
    "issues = audit.config.validate()\n",
    "if issues:\n",
    "    print(\"Configuration issues:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  {issue}\")\n",
    "else:\n",
    "    print(\"Configuration valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run the Audit\n",
    "\n",
    "Execute the fairness audit. This computes:\n",
    "\n",
    "1. **Descriptive Statistics** - Cohort characteristics\n",
    "2. **Overall Performance** - AUROC, AUPRC, calibration metrics\n",
    "3. **Subgroup Performance** - Metrics by demographic group\n",
    "4. **Fairness Assessment** - Disparity analysis with confidence intervals\n",
    "5. **Flags** - Warnings for potential issues\n",
    "6. **Governance Recommendation** - Advisory status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the audit\n",
    "results = audit.run(\n",
    "    bootstrap_ci=True,      # Compute bootstrap confidence intervals\n",
    "    n_bootstrap=1000,       # Number of bootstrap iterations\n",
    ")\n",
    "\n",
    "print(\"Audit complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Review Results\n",
    "\n",
    "### Governance Recommendation\n",
    "\n",
    "FairCareAI provides an **advisory** governance recommendation:\n",
    "\n",
    "- **READY**: No significant issues identified\n",
    "- **CONDITIONAL**: Considerations identified, recommend documentation\n",
    "- **REVIEW_REQUIRED**: Critical issues, recommend addressing before deployment\n",
    "\n",
    "**Remember**: This is CHAI-grounded guidance. Final decisions rest with your governance committee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View governance recommendation\n",
    "rec = results.governance_recommendation\n",
    "\n",
    "print(f\"Status: {rec['status']}\")\n",
    "print(f\"Advisory: {rec['advisory']}\")\n",
    "print(f\"\\nChecks: {rec['n_pass']} pass, {rec['n_warnings']} warn, {rec['n_errors']} error\")\n",
    "print(f\"\\nDisclaimer: {rec['disclaimer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View any flags\n",
    "if results.flags:\n",
    "    print(\"Flags identified:\")\n",
    "    for flag in results.flags:\n",
    "        print(f\"  [{flag['severity'].upper()}] {flag['message']}\")\n",
    "else:\n",
    "    print(\"No flags identified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View overall performance metrics\n",
    "perf = results.overall_performance\n",
    "print(\"Overall Model Performance:\")\n",
    "print(f\"  AUROC: {perf.get('auroc', {}).get('value', 'N/A'):.3f}\")\n",
    "print(f\"  AUPRC: {perf.get('auprc', {}).get('value', 'N/A'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgroup Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View subgroup performance for race/ethnicity\n",
    "print(\"Subgroup Performance (Race/Ethnicity):\")\n",
    "if \"race_ethnicity\" in results.subgroup_performance:\n",
    "    race_results = results.subgroup_performance[\"race_ethnicity\"]\n",
    "    for group, metrics in race_results.get(\"groups\", {}).items():\n",
    "        if isinstance(metrics, dict):\n",
    "            n = metrics.get(\"n\", \"?\")\n",
    "            tpr = metrics.get(\"tpr\", {}).get(\"value\", \"N/A\")\n",
    "            if isinstance(tpr, float):\n",
    "                print(f\"  {group}: n={n}, TPR={tpr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View fairness metrics\n",
    "print(\"Fairness Metrics (Race/Ethnicity):\")\n",
    "if \"race_ethnicity\" in results.fairness_metrics:\n",
    "    fairness = results.fairness_metrics[\"race_ethnicity\"]\n",
    "    print(f\"  Demographic Parity Ratios: {fairness.get('demographic_parity_ratio', {})}\")\n",
    "    print(f\"  Equalized Odds Differences: {fairness.get('equalized_odds_diff', {})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Generate Reports\n",
    "\n",
    "FairCareAI can generate reports in multiple formats:\n",
    "\n",
    "- **HTML**: Interactive dashboard for exploration\n",
    "- **PDF**: Formal audit document for records\n",
    "- **PPTX**: PowerPoint deck for board presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export reports (uncomment to generate)\n",
    "# results.to_html(\"fairness_report.html\")\n",
    "# results.to_pdf(\"fairness_report.pdf\")\n",
    "# results.to_pptx(\"governance_deck.pptx\")\n",
    "\n",
    "print(\"To generate reports, uncomment the lines above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Review with Clinical Stakeholders**: Share results with your governance committee\n",
    "2. **Document Decisions**: Record justifications for metric choices and threshold decisions\n",
    "3. **Address Flags**: Investigate and mitigate any identified disparities\n",
    "4. **Monitor in Production**: Set up ongoing fairness monitoring\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [CHAI RAIC Framework](https://www.coalitionforhealthai.org/)\n",
    "- [Fairness in ML Literature](https://fairmlbook.org/)\n",
    "- [FairCareAI Documentation](https://github.com/your-org/faircareai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Disclaimer**: FairCareAI provides CHAI-grounded guidance for fairness auditing. All outputs are advisory. Final deployment decisions rest with the health system and clinical governance committees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
